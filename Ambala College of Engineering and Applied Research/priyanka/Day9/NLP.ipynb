{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "033ecc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a16bef9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspellchecker\n",
      "  Downloading pyspellchecker-0.8.4-py3-none-any.whl.metadata (9.4 kB)\n",
      "Downloading pyspellchecker-0.8.4-py3-none-any.whl (7.2 MB)\n",
      "   ---------------------------------------- 0.0/7.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/7.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/7.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/7.2 MB 325.1 kB/s eta 0:00:23\n",
      "    --------------------------------------- 0.1/7.2 MB 655.4 kB/s eta 0:00:11\n",
      "   - -------------------------------------- 0.3/7.2 MB 1.3 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 0.4/7.2 MB 1.8 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 0.7/7.2 MB 2.7 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 1.1/7.2 MB 3.7 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 1.5/7.2 MB 4.4 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 2.0/7.2 MB 5.0 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 2.5/7.2 MB 6.0 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 3.0/7.2 MB 6.4 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 3.6/7.2 MB 7.0 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 4.1/7.2 MB 7.4 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 4.5/7.2 MB 7.6 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 5.0/7.2 MB 7.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 5.5/7.2 MB 8.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 6.1/7.2 MB 8.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 6.6/7.2 MB 8.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.2/7.2 MB 8.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.2/7.2 MB 8.3 MB/s eta 0:00:00\n",
      "Installing collected packages: pyspellchecker\n",
      "Successfully installed pyspellchecker-0.8.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pyspellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c6b6050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Using cached joblib-1.5.3-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2025.11.3-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     ---------------------------- --------- 30.7/41.5 kB 325.1 kB/s eta 0:00:01\n",
      "     -------------------------------------- 41.5/41.5 kB 332.8 kB/s eta 0:00:00\n",
      "Collecting tqdm (from nltk)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "     ---------------------------------------- 0.0/57.7 kB ? eta -:--:--\n",
      "     --------------------- ------------------ 30.7/57.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 57.7/57.7 kB 1.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\r\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.2/1.5 MB 5.0 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 0.5/1.5 MB 5.0 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 0.9/1.5 MB 7.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.3/1.5 MB 7.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.5/1.5 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 6.9 MB/s eta 0:00:00\n",
      "Downloading regex-2025.11.3-cp312-cp312-win_amd64.whl (277 kB)\n",
      "   ---------------------------------------- 0.0/277.1 kB ? eta -:--:--\n",
      "   ---------------------------------------  276.5/277.1 kB 8.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 277.1/277.1 kB 5.8 MB/s eta 0:00:00\n",
      "Downloading click-8.3.1-py3-none-any.whl (108 kB)\n",
      "   ---------------------------------------- 0.0/108.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 108.3/108.3 kB 3.1 MB/s eta 0:00:00\n",
      "Using cached joblib-1.5.3-py3-none-any.whl (309 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.5/78.5 kB 2.1 MB/s eta 0:00:00\n",
      "Installing collected packages: tqdm, regex, joblib, click, nltk\n",
      "Successfully installed click-8.3.1 joblib-1.5.3 nltk-3.9.2 regex-2025.11.3 tqdm-4.67.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script tqdm.exe is installed in 'c:\\Users\\r\\AppData\\Local\\Programs\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script nltk.exe is installed in 'c:\\Users\\r\\AppData\\Local\\Programs\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c4fa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re #regular expressions Python build in Regular Expression module(used for special scharacter removing , pattern matching , text cleaning)\n",
    "from spellchecker import SpellChecker\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6c2a76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"OMG!! ðŸ˜± I cant believed u getting the job!!! thats gr8! ðŸŽ‰ I'm so happi for u! ðŸ˜Š btw, I still remembr when we last met, u were so nervous abt the interview..lol ðŸ˜‚. Hope u come soon so we can hang out! ðŸ˜Ž Don't forget to call me 2moro!! ðŸ“ž\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7a37a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\r/nltk_data'\n    - 'c:\\\\Users\\\\r\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\nltk_data'\n    - 'c:\\\\Users\\\\r\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\r\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\r\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m tokens =\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(tokens)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\r\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[39m, in \u001b[36mword_tokenize\u001b[39m\u001b[34m(text, language, preserve_line)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mword_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m, preserve_line=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    128\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m \u001b[33;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     sentences = [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    144\u001b[39m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer.tokenize(sent)\n\u001b[32m    145\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\r\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\r\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\r\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\r\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\r\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\r/nltk_data'\n    - 'c:\\\\Users\\\\r\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\nltk_data'\n    - 'c:\\\\Users\\\\r\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\r\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\r\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "tokens =word_tokenize(text)# to create tokens from text\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae16f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\r\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\r\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\r\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')# download punkt package for tokenization\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "753b5d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['OMG', '!', '!', 'ðŸ˜±', 'I', 'cant', 'believed', 'u', 'getting', 'the', 'job', '!', '!', '!', 'thats', 'gr8', '!', 'ðŸŽ‰', 'I', \"'m\", 'so', 'happi', 'for', 'u', '!', 'ðŸ˜Š', 'btw', ',', 'I', 'still', 'remembr', 'when', 'we', 'last', 'met', ',', 'u', 'were', 'so', 'nervous', 'abt', 'the', 'interview', '..', 'lol', 'ðŸ˜‚', '.', 'Hope', 'u', 'come', 'soon', 'so', 'we', 'can', 'hang', 'out', '!', 'ðŸ˜Ž', 'Do', \"n't\", 'forget', 'to', 'call', 'me', '2moro', '!', '!', 'ðŸ“ž']\n"
     ]
    }
   ],
   "source": [
    "tokens =word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae2f7c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Tokens: ['OMG', '!', '!', 'ðŸ˜±', 'cant', 'believed', 'u', 'getting', 'job', '!', '!', '!', 'thats', 'gr8', '!', 'ðŸŽ‰', \"'m\", 'happi', 'u', '!', 'ðŸ˜Š', 'btw', ',', 'still', 'remembr', 'last', 'met', ',', 'u', 'nervous', 'abt', 'interview', '..', 'lol', 'ðŸ˜‚', '.', 'Hope', 'u', 'come', 'soon', 'hang', '!', 'ðŸ˜Ž', \"n't\", 'forget', 'call', '2moro', '!', '!', 'ðŸ“ž']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "print(\"Filtered Tokens:\", filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "625b24d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '!', 'ðŸ˜±', 'cant', 'believed', 'u', 'getting', 'job', '!', '!', '!', 'thats', 'gr8', '!', 'ðŸŽ‰', \"'m\", 'happi', 'u', '!', 'ðŸ˜Š', 'btw', ',', 'still', 'remembr', 'last', 'met', ',', 'u', 'nervous', 'abt', 'interview', '..', 'lol', 'ðŸ˜‚', '.', 'Hope', 'u', 'come', 'soon', 'hang', '!', 'ðŸ˜Ž', \"n't\", 'forget', 'call', '2moro', '!', '!', 'ðŸ“ž']\n"
     ]
    }
   ],
   "source": [
    "#remove a specific word\n",
    "word_to_remove = \"OMG\" #WORD TO REMOVE\n",
    "FILTERED_tokens = [word for word in filtered_tokens if word != word_to_remove]\n",
    "print (FILTERED_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1955212",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "#intialize the porter stemmer\n",
    "stemmer = PorterStemmer()\n",
    "#stem each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9758d4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "stammered_words = [stemmer.stem(word) for word in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f2b08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orignal Text: OMG!! ðŸ˜± I cant believed u getting the job!!! thats gr8! ðŸŽ‰ I'm so happi for u! ðŸ˜Š btw, I still remembr when we last met, u were so nervous abt the interview..lol ðŸ˜‚. Hope u come soon so we can hang out! ðŸ˜Ž Don't forget to call me 2moro!! ðŸ“ž\n",
      "Stemmed Words: ['omg', '!', '!', 'ðŸ˜±', 'i', 'cant', 'believ', 'u', 'get', 'the', 'job', '!', '!', '!', 'that', 'gr8', '!', 'ðŸŽ‰', 'i', \"'m\", 'so', 'happi', 'for', 'u', '!', 'ðŸ˜Š', 'btw', ',', 'i', 'still', 'remembr', 'when', 'we', 'last', 'met', ',', 'u', 'were', 'so', 'nervou', 'abt', 'the', 'interview', '..', 'lol', 'ðŸ˜‚', '.', 'hope', 'u', 'come', 'soon', 'so', 'we', 'can', 'hang', 'out', '!', 'ðŸ˜Ž', 'do', \"n't\", 'forget', 'to', 'call', 'me', '2moro', '!', '!', 'ðŸ“ž']\n"
     ]
    }
   ],
   "source": [
    "print(\"Orignal Text:\", text)\n",
    "print(\"Stemmed Words:\", stammed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79a7ed0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stammedtext(text):\n",
    "    tokens=word_tokenize(text)\n",
    "    from nltk.stem import PorterStemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(word) for word in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a296746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'children', 'were', 'run', 'quickli', ',', 'they', 'will', 'be', 'play', 'until', 'even', '.']\n"
     ]
    }
   ],
   "source": [
    "text2=  \"The  children were running quickly,they will be playing untill evening.\"\n",
    "stammedwords = stammedtext(text2)\n",
    "print(stammedwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10484a4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
